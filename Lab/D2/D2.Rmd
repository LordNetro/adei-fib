---
title: "D2"
output:
  pdf_document: default
  html_document: default
  word_document: default
date: '2023-11-15'
---
##introducction
This project builds upon the insights gained in the first deliverable, where we initially explored the dataset, identified key variables, and performed exploratory data analysis. In Deliverable 2, our focus shifts towards advanced statistical methods and machine learning techniques to extract deeper insights from the data. Our goal is to uncover patterns, relationships, and groupings within the dataset that go beyond basic descriptive statistics.

Key methodologies employed in this deliverable include Principal Component Analysis (PCA), K-Means Classification, Hierarchical Clustering, Multiple Correspondence Analysis (MCA), and their variations. These techniques enable us to uncover latent structures, classify vehicles based on various attributes, and identify influential factors that contribute to the variability in the dataset.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the packages

```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org"))
requiredPackages <- c("missMDA","chemometrics","mvoutlier","effects","FactoMineR","car", "factoextra","RColorBrewer","dplyr","ggmap","ggthemes","knitr", "viridis", "ggsci")
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})

rm(package.check, requiredPackages)
```
 
## Defining the working directory

We are going to charge the result of the first deliverable, so we can continue working on it without losing our work.


```{r echo = T, results = 'hide', message=FALSE, error=FALSE, warning=FALSE}
load("Cars_Data_Del1.RData")
```

```{r}
summary(df$transmission)
```


# PCA Analysis

In this section, we will delve deeper into the results of Principal Component Analysis (PCA) to provide a more comprehensive understanding. PCA is a powerful tool that helps us reduce the dimensionality of the dataset and reveal underlying patterns

```{r}
vars_dis <- c("year", "transmission", "mpg", "model", "manufacturer", "fuelType")
vars_con <- c("mileage", "tax")
vars_res <- c("price", "engineSize")

res.pca <- PCA(df[,vars_con])
```
Thanks to PCA, we can observe how mileage and tax are inversely correlated. When tax increases, mileage tends to decrease.

## Analysis of the Eigenvalues and the dominant axes

### How many axes do we have to interpret according to Kaiser 

According to Kaiser a component with an eigenvalue > 1 means that it contributes more to the variation than the original data. It is normally used as a cut-off point to determine the number of principal components that we are going to keep. Thenafter we are doing so: 

```{r}
eigenvalues <- res.pca$eig
eigenvalues
```
After calculating the eigenvalues we have checked that the first component have an eigenvalue > 1. So, we are going to keep this first dimension, having a 61.75% of cumulative variance.

```{r}
eigenvalues[1,]
rm(eigenvalues)
```

### How many axes do we have to interpret according to Elbow's rule

In the case of the Elbow's rule, we do not have a defined cutting point according to which we can decide the dimensions to use. It is then based on taking the dimensions until the difference in variance of the next factorial plane is almost the same as the current one.

With the following plot we can see wich has the minimum distance:

```{r}
fviz_screeplot(
  res.pca, 
  addlabels=TRUE, 
  ylim=c(0,70),
  barfill="lightgreen", 
  barcolor="lightgreen",
  linecolor = "darkgreen",
  main="Percentage of Variance of PCA"
)
```

With this rule we can see how the difference between the percentages is 23.4%. We are going to say that this difference is too large so that we can only get the first dimension. Thus we are obtaining the same result as with the Kaiser rule.

## Individuals point of view

### Extreme individuals

```{r}
fviz_pca_ind(res.pca, col.ind="contrib", geom = "point") +
  scale_color_gradient2(mid="lightgreen", high="red")
```
In the plot above, red points indicate individuals with high contributions. We observe that certain individuals stand out in both dimensions, suggesting they have a significant impact on the results.  

#### First Dimension

We are going to start searching for the two more extreme values:

In the first step we can find the summaries of our two variables so that we have in mind our maximums and minimums of each one so that we can compare them with the values of our individuals.

```{r}
summary(df$tax)
summary(df$mileage)
```


```{r}
rang<-order(res.pca$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
contrib.extremes
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]] | row.names(df) %in% row.names(df)[rang[1]]), 1:10]

fviz_pca_ind(res.pca, col.ind = "darkgreen", select.ind = list(names=contrib.extremes))
summary(df$tax)
```

We see that the two most influential individuals are 11864 and 33057. Comparing these results with the PCA, we see that these two elements stand out for their mileage. The first one has a value of 109050 for the mileage variable, almost the maximum, and the second one 19000.  

We are going to repeat this process but, this time for the 10 most influential individuals:

```{r}
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])

fviz_pca_ind(res.pca, col.ind = "darkgreen", select.ind = list(names=contrib.extremes))
```
In the plot above we can see the 10 individuals, in the right sight with the extreme values near to the maximum value, and the ones in the left part the ones near to the minimum. 

#### Second Dimension

Now, we are going to repeat the steps from the seccion above, but this time, with the second dimension:

```{r}
rang<-order(res.pca$ind$coord[,2])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])

df[which(row.names(df) %in% row.names(df)[rang[length(rang)]] | row.names(df) %in% row.names(df)[rang[1]]), 1:10]

fviz_pca_ind(res.pca, select.ind = list(names=contrib.extremes))
```

We can see in the grafic that this two point mentioned are 766 and 20609. If we look at the tax values of both individuals, we can indeed see that the first (766) has a tax value of 0, this being the minimum. And the second (20609) has a value of 580, beign the maximum. 

We will then do the same study for the 10 most influential individuals.

```{r}

contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])

fviz_pca_ind(res.pca, col.ind = "darkgreen", select.ind = list(names=contrib.extremes))
```

In the two directions of the second dimension we see that there are a number of individuals who are almost equally representative, although those in the negative direction are fewer and are possibly influencing the results more.

And finally, we will delete the variables as they will not be useful for the future.

```{r}
rm(contrib.extremes, rang)
```

#### Detection of multivariate outliers and influent data

Studying the data obtained in the previous section and the GWP study, we have decided that, although there are some values that are far from the rest, they are numbers that can be real, taking into account the realities they represent. That is why we conclude that we do not have multivariate outliers.

### Interpreting the axes

In this section, the aim is to study the contribution of each section in the various dimensions, and above all which are the most representative in each of them.

With the following command we will obtain the results for the two dimensions:

```{r}
res.des <- dimdesc(res.pca, axes=1:2)
res.des
```

We are going to study them separately.

##### First Dimension

```{r}
fviz_contrib(  ## contributions of variables to PC1
  res.pca, 
  fill = "lightgreen",
  color = "darkgreen",
  choice = "var", 
  axes = 1, 
  top = 6)
res.des$Dim.1
```

##### Second Dimension

```{r}
fviz_contrib(  ## contributions of variables to PC1
  res.pca, 
  fill = "lightgreen",
  color = "darkgreen",
  choice = "var", 
  axes = 2, 
  top = 6)
res.des$Dim.2
```

The analysis reveals that both the first and second dimensions capture significant variability in the data, with variables contributing substantially to each dimension. The bars reaching 50 in the contribution plots indicate the importance of variables in explaining the respective dimension's variability. 


### PCA with supplementary variables

In this last part of the PCA, we want to repeat the same analysis, but this time taking into account the supplementary variables. In our case they will be *price* and *engineSize*, as supplementary quantitative variables, and all categorical variables as qualitative variables: 


```{r}
res.pca <- PCA(df[,c(vars_con, vars_dis, vars_res)], quanti.sup = c("price", "engineSize"), quali.sup = c("year", "transmission", "mpg", "model", "manufacturer", "fuelType")) 

plot(res.pca$ind$coord[,1], res.pca$ind$coord[,2], pch=19, col="grey30", xlab = "PCA X axis", ylab = "PCA Y axis")
points(res.pca$quali.sup$coord[,1], res.pca$quali.sup$coord[,2], pch=15, col="lightgreen")
lines(res.pca$quali.sup$coord[3:4,1], res.pca$quali.sup$coord[3:4,2], lwd=2, lty=2, col="coral")
text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="lightgreen",cex=0.5)

```

```{r}
rm(res.des)
```


## K-Means Classification
### Description of clusters

We start with the K-means classification by recalculating the PCA and creating a "compPrincipals" variable with the 2 principal components according to Kaiser.

```{r}
compPrincipals <- res.pca$ind$coord[,1:2]
dim(compPrincipals)
head(compPrincipals)
```

#### Optimal number of clusters

```{r}
fviz_nbclust(compPrincipals, kmeans, method = "silhouette")
```

Running the above command we see that the optimal number of clusters using the silhouette method is 3.

### Classificaction

We calculate the distances of the principal components, and also the percentage of inertia explained:

```{r}
distCP<-dist(compPrincipals)
kmeansCP<-kmeans(distCP, 2, iter.max=30, trace=TRUE)

inercia <- 100*(kmeansCP$betweenss/kmeansCP$totss)
inercia
```

We can see that the result is an iteration and that the inertia explained is 60.3%. And we store the number of clusters in the dataframe.

```{r, warning=FALSE}
df$nClustKM<-0
df$nClustKM<-kmeansCP$cluster
df$nClustKM<-factor(df$nClustKM)
barplot(table(df$nClustKM),col="lightgreen",border="darkgreen",main="Observacions per cluster amb K-means")
rm(distCP, kmeansCP, inercia, compPrincipals)
```

#### Characteristics

To see the characteristics of each cluster, run the following command:

```{r, warning=FALSE}
dim(df)
res.cat <-catdes(df,ncol(df))
```

##### Categorical variables

We will explain the results by looking at each cluster by variable. We will start with the categorical ones.

###### Cluster 1

```{r}
res.cat$category[1]
```
The analysis of cluster 1 reveals significant patterns in various variables, with key observations as follows. Firstly, vehicles with Semi-Automatic transmission feature prominently, accounting for 82.21%, and this characteristic is highly distinctive compared to other clusters, supported by a very low p-value of 5.42e-62. Additionally, models such as T-Roc, X2, T-Cross, Arteon, and Sharan show a 100% presence, indicating an exclusive association of these models with cluster 1. Regarding fuel type, Petrol vehicles have a significant representation of 77.34%. The brands Mercedes and BMW also exhibit a prominent association with this cluster, with percentages of 71.89% and 71.52%, respectively. 

###### Cluster 2

```{r}
res.cat$category[2]
```
Cluster 2 exhibits distinct patterns in categorical variables. Notably, vehicles with Manual transmission predominate the cluster at 45.94%, indicating a strong preference for manual gear shifting. Diesel fuel type is also prevalent, constituting 37.89% of the vehicles in the cluster. Specific models such as A1, Polo, Passat, and Golf are prominently represented, suggesting a preference for these models within the cluster. Intriguingly, certain models like Q8, Caddy Maxi Life, Sharan, and Arteon show a complete absence in this cluster. The association with manufacturers VW, BMW, and Mercedes is noteworthy, with percentages of 34.92%, 28.48%, and 28.11%, respectively, suggesting a clustering effect based on brands.

##### Quantitaive variables

```{r}
res.cat$quanti.var
```
The analysis of quantitative variables reveals strong associations between the cluster structure and key features such as year of manufacture, price, mileage, tax, and MPG, with all exhibiting highly significant associations. Even the weakest predictor, engine size, shows a statistically significant relationship. These findings emphasize the pivotal role of these quantitative attributes in shaping the identified clusters, providing valuable insights into the underlying patterns in the data.


###### Cluster 1

```{r}
res.cat$quanti[1]
```
Cluster 1, as analyzed through quantitative variables, exhibits significant differences in several key aspects. The v-tests reveal substantial variations in the mean values of the variables. Notably, vehicles in Cluster 1 have a higher average year of manufacture (2018.20), lower tax rates (147.07), and a higher average price (24521.86) compared to the overall dataset. The engine size is slightly larger (1.90), while the MPG is lower (49.57), and the mileage is notably lower (12466.33). These differences are statistically significant, as indicated by the low p-values, emphasizing the distinct characteristics of vehicles in Cluster 1. 
###### Cluster 2

```{r}
res.cat$quanti[2]
```

Cluster 2, when analyzed based on quantitative variables, reveals distinctive patterns. The v-tests indicate significant differences in the mean values of the variables for this cluster. Vehicles in Cluster 2 have higher mileage (44456.09) and lower average MPG (59.83) compared to the overall dataset. Additionally, the engine size is slightly smaller (1.84), the average price is lower (13856.22), tax rates are higher (74.84), and the average year of manufacture is earlier (2015.23). These differences are statistically significant, as evidenced by the low p-values. 


## Hierarchical Clustering

To make the hierarchical clusters we will start by running the HCPC command with the number of clusters -1 to obtain the optimal number of clusters according to the programme for our case study.


```{r, warning=FALSE}
res.hcpc <- HCPC(res.pca,nb.clust = -1, order = TRUE)
```

We can see how the execution results in 4 hierarchical clusters, and in the following we will study why this is so and what characteristics each of them has.

### Description of clusters

The number of observations in each cluster is shown below:

```{r}
table(res.hcpc$data.clust$clust)
barplot(summary(res.hcpc$data.clust$clust), col="lightgreen", border="darkgreen", main="Individuals in each cluster")
```

With the barplot we can see the number of observations that each cluster has.

### Interpretation of the results of the classification

#### The description of the cluster for the variables

With the next command we can see the categorical variables that characterise the clusters:

```{r}
names(res.hcpc$desc.var)
res.hcpc$desc.var$quanti
```

We can see the intensity of the variables, only those with the smallest p-values.

In the following, we want to see for each cluster which categories characterise it.

```{r}
res.hcpc$desc.var$category
```

We now turn to the quantitative variables that characterise the clusters:

```{r}
res.hcpc$desc.var$quanti.var
```
We can see how all the variables shown are represented.

Now we want to know which variables are associated with the quantitative variables so we will run:

```{r}
res.hcpc$desc.var$quanti
```

##### Cluster 1
This cluster is characterized by significantly higher values in *mileage* and *mpg* compared to the overall average, indicating that vehicles in this cluster tend to have higher mileage and better fuel efficiency. Furthermore, *engineSize* and *tax* are lower than average, suggesting that vehicles here might have smaller engines and lower vehicle tax. The *price* and *year* are also lower, indicating that this cluster could consist of older and more economical vehicles.

##### Cluster 2
Vehicles in Cluster 2 have lower *mpg* and higher *mileage* than average, which could imply heavier usage or work-type vehicles. The *engineSize* is significantly larger, and the *tax* is much lower, suggesting that these vehicles might have larger engines but an unexpectedly low vehicle tax. The *year* and *price* are lower, indicating they could be older and less expensive vehicles.

##### Cluster 3
Cluster 3 is distinguished by having the highest values in *year*, indicating that it mainly comprises newer models. The *price* and *tax* are also high, reflecting probably the greater value and taxes associated with new vehicles. However, *mpg* is negative and *mileage* is the lowest among all clusters, which could mean that these vehicles are less used or newer.

##### Cluster 4
In Cluster 4, the *tax* variable is significantly higher, suggesting that vehicles here carry higher taxes, potentially due to higher valuations or tax rates. *engineSize* is also higher, indicating larger engine sizes. However, *mpg* and *mileage* are lower, which may indicate vehicles with less usage or fuel efficiency. The *year* is lower, suggesting that the vehicles are not the newest.

These interpretations are based on the mean and standard deviation of the quantitative variables in each cluster compared to the overall average. The p-value associated with each variable's test indicates the statistical significance of the differences found, with lower values indicating higher significance.

#### The descripction of the clusters for the individuals

```{r}
res.hcpc$desc.ind$para
```
On top we can see the most representative individuals for each cluster. From this ones we are going to obtain the rownames.

```{r}
res.hcpc$desc.ind$dist
```

What we obtain are those individuals in each cluster that are far away from the rest of the individuals in the cluster. In addition, we also get the rownames of each individual with the greatest distance from the others in the same cluster.

#### Examining the values of individuals that characterise the classes

We want to extract the graphical representation of the individuals characterising the classes (for i dist).

```{r}
para1 <- which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[1]]))
dist1<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[1]]))
para2 <- which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[2]]))
dist2<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[2]]))
para3 <- which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[3]]))
dist3<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[3]]))
para4 <- which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$para[[4]]))
dist4<-which(rownames(res.pca$ind$coord)%in%names(res.hcpc$desc.ind$dist[[4]]))
```

```{r}
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],col="grey50",cex=0.5,pch=16)
points(res.pca$ind$coord[para1,1],res.pca$ind$coord[para1,2],col="blue",cex=1,pch=16)
points(res.pca$ind$coord[dist1,1],res.pca$ind$coord[dist1,2],col="chocolate1",cex=1,pch=16)
points(res.pca$ind$coord[para2,1],res.pca$ind$coord[para2,2],col="cyan",cex=1,pch=16)
points(res.pca$ind$coord[dist2,1],res.pca$ind$coord[dist2,2],col="darkorange1",cex=1,pch=16)
points(res.pca$ind$coord[para3,1],res.pca$ind$coord[para3,2],col="deepskyblue1",cex=1,pch=16)
points(res.pca$ind$coord[dist3,1],res.pca$ind$coord[dist3,2],col="darkgoldenrod1",cex=1,pch=16)
points(res.pca$ind$coord[para4,1],res.pca$ind$coord[para3,2],col="deepskyblue1",cex=1,pch=16)
points(res.pca$ind$coord[dist4,1],res.pca$ind$coord[dist3,2],col="darkgoldenrod1",cex=1,pch=16)
```

From the grafic we can see the blue points as the para and the yellow ones the dist.

#### Quality of the partition

##### Gain in inertia (%)

```{r}
((res.hcpc$call$t$within[1]-res.hcpc$call$t$within[3])/res.hcpc$call$t$within[1])*100
```
We can see that the quality of this reduction is 59.95%.

So if we wanted more than a 80% we would need 6 clusters, as we can see below.

```{r}
((res.hcpc$call$t$within[1]-res.hcpc$call$t$within[6])/res.hcpc$call$t$within[1])*100
```

#### Saving the results in the dataframe

```{r}
res.hcpc$call$t$inert.gain[1:4]
df$hcpck<-res.hcpc$data.clust$clust
```
Cleaning
```{r}
df$nClustKM <- NULL
remove(dist1, dist2, dist3, para1, para2, para3, res.hcpc)
```

## CA Analysis

In our PCA analysis, while the primary focus has been on the variable "price," we have enriched our exploration by comparing it with categorical variables, specifically "transmission" and "fuel type." By incorporating these categorical features alongside the quantitative measure of "price," we aim to capture nuanced relationships and potential patterns that may emerge when considering both the cost aspect and the transmission and fuel type characteristics of the vehicles. Through Principal Component Analysis, we aim to distill the essential dimensions that drive the dataset's variability and gain valuable insights into its underlying structure.

```{r}
summary(df$price)
```

The groups are going to be from the minimum to the 1st Qtl, 1st Qtl to the Mean, Mean to 3rd Qtl i 3d Qtl to the Maximum:

```{r}
df$f_price[df$price<=13995] = "[1990,13995]"
df$f_price[(df$price>13995) & (df$price<=21140)] = "(13995,21140]"
df$f_price[(df$price>21140) & (df$price<=26000)] = "(21140,26000]"
df$f_price[(df$price>62995)] = "(26000,62995]"
df$f_price<-factor(df$f_price)
table(df$f_price)
```

```{r}
tt1<-table(df[,c("transmission", "f_price")]); tt1
chisq.test(tt1,  simulate.p.value = TRUE)
```

```{r}
res.ca1 <- CA(tt1)
res.ca1$eig
res.ca1$row$coord
fviz_eig(res.ca1)
```


```{r}
tt2<-table(df[,c("fuelType", "f_price")]); tt2
chisq.test(tt2,  simulate.p.value = TRUE)
```

```{r}
res.ca2 <- CA(tt2)
res.ca2$eig
res.ca2$row$coord
fviz_eig(res.ca2)
```


### Eigenvalues and dominant axes

##### Transmission

```{r}
mean(res.ca1$eig[,1])
```

##### Fuel Type


```{r}
mean(res.ca2$eig[,1])
```
Cleaning
```{r}
df$f_price <- NULL
rm(res.ca1, res.ca2, tt1, tt2)
```

## MCA Analysis

```{r, warning=FALSE}
names(df)
res.mca <- MCA(df[,c(vars_dis)])
```



```{r}
fviz_mca_ind(
  res.mca,
  geom=c("point"),
  col.ind="lightgreen"
)
```

### Eigenvalues and dominant axes

```{r}
res.mca$eig
mean(res.mca$eig[,1])
```

We can see how the mean is 0.1667 so we are going to use the X DIMENSION

```{r}
fviz_screeplot(
  res.mca, 
  addlabels=TRUE, 
  ylim=c(0,15), 
  barfill="lightgreen", 
  barcolor="darkgreen",
  linecolor="orange",
  title="Representació en cada dimensió"
)
```

### Points of view of the individuals

##### Extreme individuals

```{r}
library(factoextra)
library(viridis)

fviz_mca_ind(
  res.mca,
  geom = "point", 
  col.ind = "contrib", # Color by contribution to MCA dimensions
  gradient.cols = viridis(100) # Use the viridis color palette for continuous data
)
```

##### Groups

On this case, we are going to do the analysis with the manufacturer, year and transmission variables, as we want to see how they affect the data.
```{r}
library(factoextra)

n_colors_needed <- 21  # Number of levels in your factor
custom_palette <- colorRampPalette(c("lightgreen", "red", "purple", "blue", "yellow"))(n_colors_needed)

fviz_mca_ind(res.mca, label = "none", habillage = "manufacturer", palette = custom_palette)
fviz_mca_ind(res.mca, label = "none", habillage = "year", palette = custom_palette)
fviz_mca_ind(res.mca, label = "none", habillage = "transmission", palette = custom_palette)


```

### Average profile vs Extreme profiles

```{r}
fviz_mca_var(res.mca, choice="mca.cor", repel=TRUE)
```


### Interpretation of the assosiation of the axes

```{r}
res.desc <- dimdesc(res.mca, axes = c(1,2))
```


##### Description of the first dimension

```{r}
res.desc[[1]]
```

##### Description of the second dimension

```{r}
res.desc[[2]]
```

### MCA with supplemetary data

```{r, warning=FALSE}
res.mca <- MCA(df[,c(vars_dis, vars_res)], quanti.sup = c("price"))
```

```{r, warning=FALSE}
res.mca <- MCA(df[,c(vars_con, vars_dis, vars_res)], quanti.sup = c("mileage", "tax"))
```

#### Description of dimensions
```{r}
res.desc <- dimdesc(res.mca, axes = c(1,2))
```


##### Description of the first dimension
```{r}
res.desc[[1]]
```

##### Description of the second dimension
```{r}
res.desc[[2]]
```

Cleaning
```{r, warning=FALSE}
rm(res.desc)
```

## MCA Jerarquic Clustering

We have choosen 3 clusters as we are working with 3 groups, and we want to see the results for 3 clusters.

```{r, warning=FALSE}
res.hcmc<-HCPC(res.mca,nb.clust=3,order=TRUE)
```



### Description of the clusters

 
```{r}
table(res.hcmc$data.clust$clust)
barplot(summary(res.hcmc$data.clust$clust), col="lightgreen", border="darkgreen", main="Individuals in each cluster")
```



### Interpretation of the results of the clasification

#### The description of the cluster for the variables

Below we can see the categorical variables that caracterize the clusters.

```{r}
names(res.hcmc$desc.var)
```

We are going to start to do the description per variables with the chi squared test:

```{r}
res.hcmc$desc.var$test.chi2
```


```{r}
res.hcmc$desc.var$category[1]
```


```{r}
res.hcmc$desc.var$quanti.var
```


```{r}
res.hcmc$desc.var$quanti
```

### Quality of the participation

##### Gain in inertia

```{r}
((res.hcmc$call$t$within[1]-res.hcmc$call$t$within[3])/res.hcmc$call$t$within[1])*100
```

To have a 80% of representation in the clusters we would need 7 clusters.

```{r}
((res.hcmc$call$t$within[1]-res.hcmc$call$t$within[7])/res.hcmc$call$t$within[1])*100
```


### Parangons and specific individuals for each class

```{r}
res.hcmc$desc.ind$para
```


```{r}
res.hcmc$desc.ind$dist
```

```{r}
para1<-which(rownames(res.mca$ind$coord)%in%names(res.hcmc$desc.ind$para[[1]]))
dist1<-which(rownames(res.mca$ind$coord)%in%names(res.hcmc$desc.ind$dist[[1]]))
para2<-which(rownames(res.mca$ind$coord)%in%names(res.hcmc$desc.ind$para[[2]]))
dist2<-which(rownames(res.mca$ind$coord)%in%names(res.hcmc$desc.ind$dist[[2]]))
para3<-which(rownames(res.mca$ind$coord)%in%names(res.hcmc$desc.ind$para[[3]]))
dist3<-which(rownames(res.mca$ind$coord)%in%names(res.hcmc$desc.ind$dist[[3]]))

plot(res.mca$ind$coord[,1],res.mca$ind$coord[,2],col="grey50",cex=0.5,pch=16)
points(res.mca$ind$coord[para1,1],res.mca$ind$coord[para1,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist1,1],res.mca$ind$coord[dist1,2],col="chartreuse3",cex=1,pch=16)
points(res.mca$ind$coord[para2,1],res.mca$ind$coord[para2,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist2,1],res.mca$ind$coord[dist2,2],col="darkorchid3",cex=1,pch=16)
points(res.mca$ind$coord[para3,1],res.mca$ind$coord[para3,2],col="blue",cex=1,pch=16)
points(res.mca$ind$coord[dist3,1],res.mca$ind$coord[dist3,2],col="firebrick3",cex=1,pch=16)
```

```{r, warning=FALSE}
fviz_dend(res.hcmc, show_labels = FALSE)
```

```{r, warning=FALSE}
fviz_cluster(res.hcmc, geom = "point", main = "Factor map")
```


### Comparation clusters with focus a target duration

```{r}
df$hcmck<-res.hcmc$data.clust$clust

table(df$hcpck,df$hcmck)

```
```{r}
df$hcmck_hcpck<-factor(
  df$hcmck,
  levels=c(4,3,2,1,5),
  labels=c("kHPmca-4","kHPmca-3","kHPmca-2","kHPmca-1","kHPmca-5")
)
tt1<-table(df$hcpck,df$hcmck_hcpck); tt1
```

```{r}
100*sum(diag(tt1)/sum(tt1))
```

Cleaning
```{r}
rm(dist1, dist2, dist3, para1, para2, para3, tt1)
rm(res.hcmc, res.pca, res.mca, vars_con, vars_dis, vars_res)
df$hcpck <- NULL
df$hcmck <- NULL
df$hcmck_hcpck <- NULL
```

## Conclusions

Throughout Deliverable 2, we have employed advanced statistical techniques to further analyze the dataset introduced in our first deliverable. Employing Principal Component Analysis (PCA), we discerned inverse correlations between mileage and tax, highlighting an important aspect of our dataset. The Eigenvalues and Scree plots informed our decision to retain the first dimension in PCA due to its significant contribution to variance.

In our exploration of extreme individuals, we identified specific vehicles that significantly influence our PCA results, providing deeper insights into the dataset. Our analysis did not detect multivariate outliers, suggesting that the extreme values identified are plausible within the context of the data.

The axes interpretation segment revealed the substantial impact of categorical variables on the dataset's structure. Subsequently, the inclusion of supplementary variables in our PCA brought forth enriched perspectives, although some visualizations appeared unconventional.

The Hierarchical Clustering on Principal Components (HCPC) allowed us to delineate four distinct clusters, offering a nuanced understanding of the dataset's segmentation. The interpretation of these clusters highlighted significant differences in vehicle characteristics such as mileage, tax, price, engine size, and year of manufacture.

For MCA, we chose to visualize the association of manufacturer, year, and transmission with the data points, giving us a clear view of how these variables segment the dataset. The hierarchical clustering within MCA identified three clusters, which, upon evaluation, did not meet the 80% representation threshold, suggesting the need for additional clusters.

The final part of the deliverable involved comparing the clusters from PCA and MCA with a focus on a target duration, illustrating the degree of alignment between these classifications. The process culminated in the export of the refined dataset for subsequent analysis in Deliverable 3.

This deliverable has significantly advanced our understanding of the dataset, setting the stage for even deeper analyses that will be undertaken in Deliverable 3.

```{r}
save(df, file = "Cars_Data_Del2.RData")
``` 

With Deliverable 2 completed, we now possess a robust foundation for the continuation of our work, having solidified our grasp of the dataset's underlying patterns and relationships. The methodologies applied have equipped us with actionable insights, enabling a more informed approach to our next deliverable.
